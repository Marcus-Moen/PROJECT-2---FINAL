{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e595b8cf",
   "metadata": {},
   "source": [
    "# **AIRLINE REVIEW SENTIMENT ANALYSIS**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d1c17b",
   "metadata": {},
   "source": [
    "### **1. BUSINESS PROBLEM**\n",
    "\n",
    "\n",
    "In today's highly competitive and customer-centric market, businesses receive a lot of customer feedback through reviews across many different platforms. However, manually analyzing this data is time-consuming and often inconsistent. Our application addresses this problem by using sentiment analysis to automatically categorize customer reviews as positive, negative, or neutral. This enables businesses to quickly identify areas of improvement, monitor brand perception in real-time, and make data-driven decisions to enhance customer satisfaction and loyalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0435035a",
   "metadata": {},
   "source": [
    "### **2. PROBLEM STATEMENT**\n",
    "\n",
    "ADD CONTENT HERE..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1fc102",
   "metadata": {},
   "source": [
    "### **3. DATA UNDERSTANDING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2275737",
   "metadata": {},
   "source": [
    "Visualize as much of the dataset as possible in terms of distribution, relationships between variables and so forth to get a solid understanding of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcfe8a1",
   "metadata": {},
   "source": [
    "### **4. DATA PREPARATION & TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "889eed46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marcu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marcu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\marcu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\marcu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords', download_dir='C:\\\\Users\\\\marcu\\\\AppData\\\\Roaming\\\\nltk_data')\n",
    "nltk.download('punkt', download_dir='C:\\\\Users\\\\marcu\\\\AppData\\\\Roaming\\\\nltk_data')\n",
    "nltk.download('wordnet', download_dir='C:\\\\Users\\\\marcu\\\\AppData\\\\Roaming\\\\nltk_data')\n",
    "nltk.download('omw-1.4', download_dir='C:\\\\Users\\\\marcu\\\\AppData\\\\Roaming\\\\nltk_data')  # Needed for lemmatization\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"Tweets.csv\")\n",
    "\n",
    "# Keep only relevant columns\n",
    "df = df[['airline_sentiment', 'text']]\n",
    "\n",
    "# Load stopwords once\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# stemmer = PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    try:\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "        text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        tokens = text.split()\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        return ' '.join(tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning text: {text} -> {e}\")\n",
    "        return \"\"\n",
    "\n",
    "try:\n",
    "    df['clean_text'] = df['text'].apply(clean_text)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['sentiment_encoded'] = label_encoder.fit_transform(df['airline_sentiment'])\n",
    "\n",
    "# Vectorize text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), lowercase=False)\n",
    "\n",
    "# Added bi-grams to help the model understand context better\n",
    "# A bi-gram is a sequence of two adjacent elements from a string of tokens, which can help capture context better than unigrams alone.\n",
    "X = vectorizer.fit_transform(df['clean_text']).toarray()\n",
    "\n",
    "# Define features and labels\n",
    "y = df['sentiment_encoded']\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e967eb",
   "metadata": {},
   "source": [
    "### **5. EVALUATING DIFFERENT TYPES OF MODELS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c056604",
   "metadata": {},
   "source": [
    "##### **LOGISTIC REGRESSION**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9c4c7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.7991803278688525\n",
      "Logistic Regression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.94      0.87      1889\n",
      "           1       0.67      0.50      0.57       580\n",
      "           2       0.82      0.61      0.70       459\n",
      "\n",
      "    accuracy                           0.80      2928\n",
      "   macro avg       0.77      0.68      0.72      2928\n",
      "weighted avg       0.79      0.80      0.79      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize and train Logistic Regression model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_logreg))\n",
    "print(\"Logistic Regression Classification Report:\\n\", classification_report(y_test, y_pred_logreg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2e44bb",
   "metadata": {},
   "source": [
    "##### **TRAINING A MODEL USING RANDOM FOREST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a384248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# # Initialize and train Random Forest model\n",
    "# rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf.fit(X_train, y_train)\n",
    "\n",
    "# # Predictions\n",
    "# y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# # Evaluation\n",
    "# print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "# print(\"Random Forest Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4f09e6",
   "metadata": {},
   "source": [
    "##### **COMPARING XGBOOST & LOGISTIC REGRESSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67d39f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marcu\\OneDrive\\MarcusOneDrive\\IT Projects\\PROJECT 2 - FINAL\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:38:32] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.7991803278688525\n",
      "Logistic Regression Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.94      0.87      1889\n",
      "           1       0.67      0.50      0.57       580\n",
      "           2       0.82      0.61      0.70       459\n",
      "\n",
      "    accuracy                           0.80      2928\n",
      "   macro avg       0.77      0.68      0.72      2928\n",
      "weighted avg       0.79      0.80      0.79      2928\n",
      "\n",
      "XGBoost Accuracy: 0.7612704918032787\n",
      "XGBoost Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.94      0.85      1889\n",
      "           1       0.66      0.30      0.41       580\n",
      "           2       0.73      0.61      0.67       459\n",
      "\n",
      "    accuracy                           0.76      2928\n",
      "   macro avg       0.72      0.62      0.64      2928\n",
      "weighted avg       0.75      0.76      0.73      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(eval_metric='mlogloss', use_label_encoder=False)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_logreg))\n",
    "print(\"Logistic Regression Classification Report:\\n\", classification_report(y_test, y_pred_logreg))\n",
    "\n",
    "# Evaluate XGBoost\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"XGBoost Classification Report:\\n\", classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cdb767",
   "metadata": {},
   "source": [
    "##### **TRAINING USING VADER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db9e91ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\marcu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon', download_dir='C:\\\\Users\\\\marcu\\\\AppData\\\\Roaming\\\\nltk_data')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# VADER prediction function\n",
    "def get_vader_sentiment(text):\n",
    "    score = sia.polarity_scores(text)['compound']\n",
    "    if score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "    \n",
    "    # Get the original text of your test set (aligning indexes with y_test)\n",
    "X_test_text = df.loc[y_test.index, 'text']\n",
    "\n",
    "# Get VADER predictions for each tweet\n",
    "vader_preds = X_test_text.apply(get_vader_sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12118b9b",
   "metadata": {},
   "source": [
    "##### **COMPARING VADER & LOGISTIC REGRESSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "461b5a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML Model Accuracy: 0.7991803278688525\n",
      "ML Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.94      0.87      1889\n",
      "     neutral       0.67      0.50      0.57       580\n",
      "    positive       0.82      0.61      0.70       459\n",
      "\n",
      "    accuracy                           0.80      2928\n",
      "   macro avg       0.77      0.68      0.72      2928\n",
      "weighted avg       0.79      0.80      0.79      2928\n",
      "\n",
      "\n",
      "VADER Accuracy: 0.5502049180327869\n",
      "VADER Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.51      0.65      1889\n",
      "     neutral       0.36      0.43      0.39       580\n",
      "    positive       0.34      0.88      0.49       459\n",
      "\n",
      "    accuracy                           0.55      2928\n",
      "   macro avg       0.54      0.60      0.51      2928\n",
      "weighted avg       0.71      0.55      0.57      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Decode your model's predictions (you already trained your ML model above)\n",
    "ml_preds_labels = label_encoder.inverse_transform(y_pred_logreg)\n",
    "y_test_labels = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "# Compare results\n",
    "print(\"ML Model Accuracy:\", accuracy_score(y_test_labels, ml_preds_labels))\n",
    "print(\"ML Classification Report:\\n\", classification_report(y_test_labels, ml_preds_labels))\n",
    "\n",
    "print(\"\\nVADER Accuracy:\", accuracy_score(y_test_labels, vader_preds))\n",
    "print(\"VADER Classification Report:\\n\", classification_report(y_test_labels, vader_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0903cfc",
   "metadata": {},
   "source": [
    "##### **TRAINING AN MLP TO COMPARE WITH LOGISTIC REGRESSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68f6043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# # Initialize and train MLP model\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)  # You can tune the hidden_layer_sizes\n",
    "# mlp.fit(X_train, y_train)\n",
    "\n",
    "# # Predictions\n",
    "# y_pred_mlp = mlp.predict(X_test)\n",
    "\n",
    "# # Evaluate MLP\n",
    "# print(\"MLP Accuracy:\", accuracy_score(y_test, y_pred_mlp))\n",
    "# print(\"MLP Classification Report:\\n\", classification_report(y_test, y_pred_mlp))\n",
    "\n",
    "# # Compare the results with Logistic Regression (already done in your code)\n",
    "# print(\"\\nML Model Accuracy:\", accuracy_score(y_test, y_pred_logreg))\n",
    "# print(\"ML Classification Report:\\n\", classification_report(y_test, y_pred_logreg))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6337b4b2",
   "metadata": {},
   "source": [
    "##### **SAVING THE MODEL FILE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bdac7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = LogisticRegression()\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model and vectorizer\n",
    "import pickle\n",
    "\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "with open('vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
